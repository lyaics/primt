<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PRIMT: Preference-based Reinforcement Learning</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fdfdfd;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 900px;
            margin: 20px auto;
            padding: 0 20px;
        }
        header, section {
            margin-bottom: 40px;
        }
        h1, h2, h3 {
            text-align: center;
            font-weight: 400;
            color: #222;
        }
        h1 {
            font-size: 2em;
            margin-bottom: 10px;
        }
        h2 {
            font-size: 1.6em;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
            margin-top: 50px;
        }
        h3 {
            font-size: 1.3em;
            margin-top: 40px;
            color: #444;
        }
        .authors, .affiliations, .conference {
            text-align: center;
            margin-bottom: 10px;
        }
        .authors {
            font-size: 1.1em;
        }
        .affiliations {
            font-style: italic;
            color: #555;
        }
        .conference {
            font-size: 1.1em;
            font-weight: bold;
        }
        .links {
            text-align: center;
            margin: 25px 0;
        }
        .links a {
            display: inline-block;
            margin: 0 10px;
            padding: 10px 20px;
            background-color: #337ab7;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            font-size: 1.1em;
            transition: background-color 0.3s;
        }
        .links a:hover {
            background-color: #286090;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        p {
            font-size: 1em;
            text-align: justify;
        }
        .caption {
            text-align: center;
            font-style: italic;
            color: #666;
            margin-top: -10px;
        }
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            text-align: center;
        }
        .result-item h4 {
            margin-bottom: 10px;
            font-size: 1.1em;
            font-weight: bold;
        }
        .bibtex {
            background-color: #f0f0f0;
            border: 1px solid #ccc;
            padding: 15px;
            border-radius: 5px;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-family: "Courier New", Courier, monospace;
            font-size: 0.9em;
        }
    </style>
</head>
<body>

    <div class="container">

        <header>
            <h1>PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models</h1>
            
            <div class="authors">
                Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Tianyu Shao, Guohua Chen, Dominic Kao, Sungeun Hong, and Byung-Cheol Min
            </div>

            <div class="affiliations">
                SMART Lab, Purdue University
            </div>

            <div class="conference">
                NeurIPS 2025 (Oral Presentation)
            </div>

            <div class="links">
                <a href="#" target="_blank">Paper</a>
                <a href="#" target="_blank">Video</a>
                </div>
        </header>

        <section id="abstract">
            <h2>Abstract</h2>
            <p>
                Preference-based reinforcement learning (PBRL) shows promise in aligning robot behaviors with human preferences, but its success depends heavily on the accurate modeling of human preferences through reward models. 
                Most methods adopt Markovian assumptions for preference modeling (PM) which overlook the temporal dependencies within robot behavior trajectories that impact human evaluations. 
                While recent works have utilized sequence modeling to mitigate this by learning sequential non-Markovian rewards, they ignore the multimodal nature of robot trajectories, which consist of elements from two distinctive modalities: state and action. 
                As a result, they often struggle to capture the complex interplay between these modalities that significantly shapes human preferences. In this paper, we propose a multimodal sequence modeling approach for PM by disentangling state and action modalities. 
                We introduce a multimodal transformer network, named PrefMMT, which hierarchically leverages intra-modal temporal dependencies and inter-modal state-action interactions to capture complex preference patterns. 
                We demonstrate that PrefMMT consistently outperforms state-of-the-art PM baselines on locomotion tasks from the DMControl benchmark and manipulation tasks from the Meta-World benchmark.
            </p>
        </section>

        <section id="framework">
            <h2>Framework Overview</h2>
            <img src="assets/PRIMT_framework_animation.gif" alt="PRIMT Framework Overview Diagram">
            <p class="caption">
                Overview of PRIMT, comprising two synergistic modules: 1) Hierarchical neuro-symbolic preference fusion improves the quality and reliability of synthetic feedback by leveraging the complementary collective intelligence of VLMs and LLMs for multimodal evaluation of robot behaviors, and 2) Bidirectional trajectory synthesis consists of foresight trajectory generation, which bootstraps the trajectory buffer to mitigate early-stage query ambiguity, and hindsight trajectory augmentation, which applies SCM-based counterfactual reasoning to improve reward learning with a causal auxiliary loss that enables fine-grained credit assignment.
            </p>
        </section>

        <section id="demos">
            <h2>Experimental Demos</h2>
            <p style="text-align: center;">
                PRIMT can lead to more efficient robot behaviors across diverse tasks in zero-shot.
            </p>

            <h3>Simulation Environments</h3>
            <div class="results-grid">
                <div class="result-item">
                    <h4>MetaWorld</h4>
                    <img src="assets/metaworld_demo.gif" alt="MetaWorld Demo">
                </div>
                <div class="result-item">
                    <h4>ManiSkill</h4>
                    <img src="assets/maniskill_demo.gif" alt="ManiSkill Demo">
                </div>
                <div class="result-item">
                    <h4>DMControl</h4>
                    <img src="assets/dmcontrol_demo.gif" alt="DMControl Demo">
                </div>
            </div>
            
            <h3>Real-world Deployment</h3>
            <div class="results-grid">
                 <div class="result-item">
                    <h4>Block Lifting</h4>
                    <img src="assets/real_lifting.gif" alt="Real-world Block Lifting">
                </div>
                <div class="result-item">
                    <h4>Block Stacking</h4>
                    <img src="assets/real_stacking.gif" alt="Real-world Block Stacking">
                </div>
            </div>
        </section>
        
        <section id="details">
            <h2>More Details</h2>
            <p style="text-align: center;">Additional details on our prompting and trajectory synthesis methods.</p>
            <div class="results-grid">
                <div class="result-item">
                    <h4>LLM Evaluation Prompt</h4>
                    <img src="assets/prompt_llm.gif" alt="LLM Evaluation Prompt Template">
                </div>
                 <div class="result-item">
                    <h4>VLM Evaluation Prompt</h4>
                    <img src="assets/prompt_vlm.gif" alt="VLM Evaluation Prompt Template">
                </div>
                <div class="result-item">
                    <h4>Trajectory Synthesis</h4>
                    <img src="assets/trajectory_synthesis.gif" alt="Trajectory Synthesis Details">
                </div>
            </div>
        </section>

        <section id="citation">
            <h2>BibTeX</h2>
            <pre class="bibtex"><code>@inproceedings{wang2025primt,
    title     = {PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models},
    author    = {Wang, Ruiqi and Zhao, Dezhong and Yuan, Zigin and Shao, Tianyu and Chen, Guohua and Kao, Dominic and Hong, Sungeun and Min, Byung-Cheol},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    year      = {2025}
}</code></pre>
        </section>

    </div>

</body>
</html>
